# -*- coding: utf-8 -*-
"""A1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14TumPy0Wlw-1mIRB2ylsC6tJe7SshY3s
"""

import numpy as np
import numpy.random as rnd
import time
import pickle
import matplotlib.pyplot as plt
import sklearn.linear_model as lin
import sklearn.neighbors as ne

#from google.colab import drive
#drive.mount('/content/gdrive')

"""1)"""

rnd.seed(3)

print('\n\nQuestion 1')
print('----------')

print('\nQuestion 1(a):')
B = rnd.rand(4,5)
print(B)

print('\nQuestion 1(b):')
y = rnd.rand(4,1)
print(y)

print('\nQuestion 1(c):')
C = np.reshape(B, (2,10))
print(C)

print('\nQuestion 1(d):')
D = B - y
print(D)

print('\nQuestion 1(e):')
z = np.reshape(y, (4))
print(z)

print('\nQuestion 1(f):')
B[:,3] = z
print(B)

print('\nQuestion 1(g):')
D[:,0] = B[:,2] + z
print(D)

print('\nQuestion 1(h):')
print(B[1:4,:])

print('\nQuestion 1(i):')
print(B[:, 1:4])

print('\nQuestion 1(j):')
print(np.log(B))

print('\nQuestion 1(k):')
print(np.sum(B))

print('\nQuestion 1(l):')
print(np.max(B, axis=0))

print('\nQuestion 1(m):')
print(np.max(np.sum(B, axis=1)))

print('\nQuestion 1(n):')
print(np.matmul(np.transpose(B), D))

print('\nQuestion 1(o):')
print(np.matmul(np.matmul(np.matmul(np.transpose(y), D), np.transpose(D)), y))

"""2)"""

print('\n\nQuestion 2')
print('----------')

"""a)"""

#a)
def mult(c, d):
    e = np.zeros((c.shape[0], d.shape[1]))

    for i in range(c.shape[1]):
        for j in range(d.shape[0]):
            for k in range(c.shape[1]):
                e[i,j] += c[i,k] * d[k, j]
    
    return e  

'''
a = np.ones((10,10))
b = np.ones((10,10))
print(mult(a,b))'''

def add(c, d):
    e = c

    for i in range(e.shape[0]):
        for j in range(e.shape[1]):
            e[i,j] += d[i,j]

    return e

'''
a = np.ones((5,5))
b = np.ones((5,5))
print(add(a,b))'''

def matrix_poly(A):
    B1 = mult(A, A)
    #print(B1)
    B1 = A + B1
    #print(B1)
    B1 = mult(A, B1)
    #print(B1)
    B1 = A + B1
    #print(B1)
    return B1

"""b)"""

#b)
def timing(N):
    #A = np.ones((N,N))
    A = rnd.rand(N,N)

    t = time.time()
    B1 = matrix_poly(A)
    print("B1 Time: {0}".format(time.time()-t))

    t = time.time()
    B2 = np.matmul(A,A)
    #print(B2)
    B2 = A + B2
    #print(B2)
    B2 = np.matmul(A, B2)
    #print(B2)
    B2 = A+ B2
    #print(B2)
    print("B2 Time: {0}".format(time.time()-t))

    mag = np.linalg.norm(B1 - B2)
    print("Magnitude of distance: {0}".format(mag))

    #print(B1)
    #print(B2)


print('\nQuestion 2(c):')
for i in [100, 300, 1000]:
  print("\n\nTiming: {0}".format(i))
  timing(i)

"""3"""

print('\n\nQuestion 3')
print('----------')

file_path = 'dataA1Q3.pickle'

with open(file_path, 'rb') as f:
  dataTrain, dataTest = pickle.load(f)

x = dataTrain[0]
t = dataTrain[1]

"""a)"""

def least_squares(x, t):
  X = np.ones((x.shape[0],2))
  X[:, 1] = x

  P = np.matmul(np.transpose(X), X)
  P = np.linalg.inv(P)
  P = np.matmul(P, np.transpose(X))
  w = np.matmul(P, t)

  return (w[0], w[1])

"""b)"""

#b)
def plot_data(x,t):
  b,a = least_squares(x,t)
  plt.scatter(x,t)
  
  X = np.linspace(min(x), max(x), 2)
  Y = a*X + b
  plt.plot(X,Y)

  plt.title("Question 3b: the fitted line")
  plt.show()

  return a,b

"""c)"""

#c)
def error(a, b, X, T):
  SSE = (T - (a*X + b)) ** 2
  MSE = np.mean(SSE)
  
  return MSE

"""d)"""

print('\nQuestion 3(d):')
a,b = plot_data(dataTrain[0], dataTrain[1])

print("a = {0}, b = {1}".format(a,b))

print(error(a,b, dataTrain[0], dataTrain[1]))
print(error(a,b, dataTest[0], dataTest[1]))

"""4"""

print('\n\nQuestion 4')
print('----------')

file_path = 'dataA1Q4v2.pickle'
with open(file_path, 'rb') as f:
  Xtrain, Ttrain, Xtest, Ttest = pickle.load(f)

#from google.colab import files
#files.upload()

import bonnerlib3 as bl3d
bl3d.plot_data(Xtrain, Ttrain, 30, 10)

"""a)"""

print('\nQuestion 4(a):')
clf = lin.LogisticRegression()    # create a classification object, clf
clf.fit(Xtrain,Ttrain)            # learn a logistic-regression classifier
W = clf.coef_[0]                  # weight vector
W0 = clf.intercept_[0]            # bias term

print(W)
print(W0)

"""b)"""

print('\nQuestion 4(b):')
accuracy1 = clf.score(Xtest, Ttest)

Z = np.matmul(W, np.transpose(Xtest)) + W0
Y = 1/(1+np.exp(-Z))
Y = np.where(Y>0.5, 1,0)

accuracy2 = np.mean(np.where(Y==Ttest, 1, 0))

print("accuracy1 = {0} \naccuracy2 = {1}".format(accuracy1, accuracy2))
print("accuracy1 - accuracy2 = {0}".format(accuracy1-accuracy2))

"""c)"""

#c)
bl3d.plot_db(Xtrain, Ttrain, W, W0, elevation=30, azimuth=5)
plt.suptitle("Question 4(c): Training data and decision boundary")

"""d)"""

#d)
bl3d.plot_db(Xtrain, Ttrain, W, W0, elevation=30, azimuth=20)
plt.suptitle("Question 4(d): Training data and decision boundary")

"""5"""

print('\n\nQuestion 5')
print('----------')

def sigmoid(z):
  return 1/(1+np.exp(-z))

def cross_entropy(y,t):
  return (-t * np.log(1-y)) - (1-t)*np.log(1-y)

def gd_logreg(lrate):
  global Xtrain
  global Xtest
  global Ttrain
  global Ttest
  global W
  global W0
  N = Xtrain.shape[0]
  m = Xtrain.shape[1]

  o = np.ones((N,1))
  Xtrain = np.concatenate((Xtrain,o), axis=1)

  o = np.ones((Xtest.shape[0], 1))
  Xtest = np.concatenate((Xtest, o), axis=1)

  #a)
  np.random.seed(3)

  #b)
  w = np.random.randn(m+1) / 1000

  #c)
  ce_train  = []
  acc_train = []
  ce_test   = []
  acc_test  = []

  #d)
  iter = 0
  while (iter < 2 or np.absolute(ce_train[-2] - ce_train[-1]) > 10**-10):
    z = np.matmul(w, np.transpose(Xtrain))
    y = sigmoid(z)
    w = w - (lrate/N) * np.matmul(np.transpose(Xtrain), y-Ttrain)

    #training_loss + accuracy
    loss = np.mean(cross_entropy(y, Ttrain))
    ce_train.append(loss)
    pred = np.where(y>0.5,1,0)
    accuracy = np.mean(np.where(pred==Ttrain,1,0))
    acc_train.append(accuracy)

    #test loss + accuracy
    test_z = np.matmul(w, np.transpose(Xtest))
    test_y = sigmoid(test_z)
    test_loss = np.mean(cross_entropy(test_y, Ttest))
    ce_test.append(test_loss)
    test_pred = np.where(test_y>0.5,1,0)
    test_accuracy = np.mean(np.where(test_pred==Ttest,1,0))
    acc_test.append(test_accuracy)

    '''
    print("Iter: {0}".format(iter+1))
    print("Training cost: {0}; Training accuracy: {1}".format(loss,accuracy))
    print("Test cost:     {0}; Test accuracy    : {1}".format(np.mean(ce_test), test_accuracy))
    print()'''
    iter+=1

  #e)
  print("Weight vector:")
  print(w)
  print("Iterations: {0}".format(iter))
  print("The weight learned in question 4 was:\n{0}".format(W))
  print("The bias learned in question 4 was: {0}".format(W0))

  #f)
  plt.figure(0)
  plt.plot(ce_train)
  plt.plot(ce_test, 'r')
  plt.suptitle("Question 5: Training and test loss v.s. iterations")
  plt.xlabel("Iteration number")
  plt.ylabel("Cross entropy")

  #g)
  plt.figure(1)
  plt.semilogx(ce_train)
  plt.semilogx(ce_test, 'r')
  plt.suptitle("Question 5: Training and test loss v.s. iterations (log scale)")
  plt.xlabel("Iteration number")
  plt.ylabel("Cross entropy")

  #h)
  plt.figure(2)
  plt.semilogx(acc_train)
  plt.semilogx(acc_test, 'r')
  plt.suptitle("Question 5: Training and test accuracy v.s. iterations (log scale)")
  plt.xlabel("Iteration number")
  plt.ylabel("Accuracy")

  #i)
  plt.figure(3)
  plt.plot(ce_train[-100:])
  plt.suptitle("Question 5: last 100 training cross entropies")
  plt.xlabel("Iteration number")
  plt.ylabel("Cross entropy")

  #j)
  plt.figure(4)
  plt.semilogx(ce_train[50:])
  plt.suptitle("Question 5: test loss from iteration 50 on (log scale)")
  plt.xlabel("Iteration number")
  plt.ylabel("Cross entropy")

  #k)
  bl3d.plot_db(Xtrain, Ttrain, w[:-1], w[-1],  30, 5)
  plt.suptitle("Question 5: Training data and decision boundary")

gd_logreg(0.3)

"""6"""

print('\n\nQuestion 6')
print('----------')

file_path = 'mnistTVT.pickle'
with open(file_path, 'rb') as f:
  Xtrain, Ttrain, Xval, Tval, Xtest, Ttest = pickle.load(f)

"""a)"""

#a)
def extract(X, T, num1, num2):
  t1 = T == num1
  t2 = T == num2
  t  = np.not_equal(t1, t2)

  return X[t], T[t]

xtrain, ttrain = extract(Xtrain, Ttrain, 5, 6)
xtrain_small = xtrain[:2000]
ttrain_small = ttrain[:2000]
xval, tval = extract(Xval, Tval, 5, 6)
xtest, ttest = extract(Xtest, Ttest, 5, 6)

"""b)"""

print('\nQuestion 6(b):')
def display(x):
  display = x[:16,:]
  display = np.reshape(display, (4, 4, 28, 28))
  f, axarr = plt.subplots(4,4)
  for i in range(4):
    axarr[i,0].imshow(display[i,0], cmap='Greys', interpolation='nearest')
    axarr[i,0].axis('off')
    axarr[i,1].imshow(display[i,1], cmap='Greys', interpolation='nearest')
    axarr[i,1].axis('off')
    axarr[i,2].imshow(display[i,2], cmap='Greys', interpolation='nearest')
    axarr[i,2].axis('off')
    axarr[i,3].imshow(display[i,3], cmap='Greys', interpolation='nearest')
    axarr[i,3].axis('off')
  plt.suptitle("Question 6(b): 16 MNIST training images")    

display(xtrain)

"""c)"""

print('\nQuestion 6(c):')

"""i)"""

#i)
def test_Ks(xtrain, ttrain, xval, tval, xtrain_small, ttrain_small):
  Train_acc = []
  Val_acc   = []
  iters     = []
  for i in range(1,20,2):
    model = ne.KNeighborsClassifier(n_neighbors=i)
    model.fit(xtrain, ttrain)
    
    Val_acc.append(model.score(xval, tval))
    Train_acc.append(model.score(xtrain_small, ttrain_small))
    iters.append(i)

  return Train_acc, Val_acc, iters

Train_acc, Val_acc, iters = test_Ks(xtrain, ttrain, xval, tval, xtrain_small, ttrain_small)

"""ii)"""

#ii)
plt.plot(iters, Train_acc)
plt.plot(iters, Val_acc, 'r')
plt.suptitle("Question 6(c): Training and Validation for KNN, digits 5 and 6")
plt.xlabel("Number of Neighbors, K")
plt.ylabel("Accuracy")

"""iii)"""

max(Val_acc)

"""iv)"""

model = ne.KNeighborsClassifier(n_neighbors=3)
model.fit(xtrain, ttrain)
Test_acc = model.score(xtest, ttest)

"""v)"""

print("The best value of K is {0}".format(3))

"""vi)"""

print("Validation accuracy: {0}".format(Val_acc[1]))
print("Test accuracy      : {0}".format(Test_acc))

"""d)"""

print('\nQuestion 6(d):')

#a)
xtrain, ttrain = extract(Xtrain, Ttrain, 4, 7)
xval,   tval   = extract(Xval, Tval, 4, 7)
xtest, ttest   = extract(Xtest, Ttest, 4, 7)

xtrain_small = xtrain[:2000]
ttrain_small = ttrain[:2000]

#b)
display(xtrain)
plt.suptitle("Question 6(d): MNIST training images")

#c)
Train_acc, Val_acc, iters = test_Ks(xtrain, ttrain, xval, tval, xtrain_small, ttrain_small)

plt.plot(iters, Train_acc)
plt.plot(iters, Val_acc, 'r')
plt.suptitle("Question 6(d): Training and Validation for KNN, digits 4 and 7")
plt.xlabel("Number of Neighbors, K")
plt.ylabel("Accuracy")

print(max(Val_acc))

model = ne.KNeighborsClassifier(n_neighbors=3)
model.fit(xtrain, ttrain)
Test_acc = model.score(xtest, ttest)

print("The best value of K is {0}".format(2 * Val_acc.index(max(Val_acc)) + 1))

print("Validation accuracy: {0}".format(Val_acc[1]))
print("Test accuracy      : {0}".format(Test_acc))

"""e)"""

#e) 
'''
I suspect that the biggest reason behind both of these occurences is the fact 
that there are more differences between 4 and 7 then between 5 and 6. A 5 is 
more likely to look similar to a 6 and vice versa than a 4 is to a 7. 
'''

"""f)"""

#f)
'''
because we only have two classes, an odd number of k prevents a tie in the 
decision. In each prediction, one class will hold the majority of predictions
'''

"""g)"""

#g)
'''
Because numbers were designed to look different from one another and 
are unlikely to have many outliers in comparison to some other data sets. This 
lack of outliers makes it a great candidate for KNN because any given handdrawn
digit is highly likely to look like another hand drawn version of the same
digit
'''